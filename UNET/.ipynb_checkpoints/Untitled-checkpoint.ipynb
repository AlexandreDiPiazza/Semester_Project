{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e2c20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import gc\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "x_train1 = np.load('X_Data_NoGAN_CNN.npy', allow_pickle = True)\n",
    "x_train1_valid = np.load('X_Data_NoGAN_CNN_valid.npy', allow_pickle = True)\n",
    "y_train1 = np.load('Y_Data_NoGAN_CNN.npy', allow_pickle = True)\n",
    "y_train1_valid = np.load('Y_Data_NoGAN_CNN_valid.npy', allow_pickle = True)\n",
    "\n",
    "x_train2 = np.load('X_Data_GAN_CNN.npy', allow_pickle = True)\n",
    "x_train2_valid = np.load('X_Data_GAN_CNN_valid.npy', allow_pickle = True)\n",
    "y_train2 = np.load('Y_Data_GAN_CNN.npy', allow_pickle = True)\n",
    "y_train2_valid = np.load('Y_Data_GAN_CNN_valid.npy', allow_pickle = True)\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters = 32, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=2, min_lr=1e-5, verbose=1),\n",
    "]\n",
    "\n",
    "\n",
    "def jaccard_distance(y_true, y_pred):\n",
    "    \"\"\" Calculates mean of Jaccard distance as a loss function \"\"\"\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1,2))\n",
    "    sum_ = tf.reduce_sum(y_true + y_pred, axis=(1,2))\n",
    "    jac = (intersection + 100.) / (sum_ - intersection + 100.)\n",
    "    jd =  (1 - jac) * 100.\n",
    "    return tf.reduce_mean(jd)\n",
    "im_height = 256 \n",
    "im_width = 256\n",
    "\n",
    "input_img = Input((im_height, im_width, 1), name='img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1169a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1 = get_unet(input_img, n_filters= 32, dropout=0.1, batchnorm=True)\n",
    "model1.compile(optimizer=Adam(lr =1e-2), loss=\"binary_crossentropy\", metrics=[jaccard_distance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54b12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 62s 658ms/step - loss: 0.0265 - jaccard_distance: 30.6381\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,jaccard_distance\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,jaccard_distance,lr\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    }
   ],
   "source": [
    "im_height = 64 \n",
    "im_width = 64\n",
    "\n",
    "results_1 = model1.fit(x_train1[:,:,:], y_train1[:,:,:], batch_size=10, epochs=1, \\\n",
    "                    callbacks= callbacks + [ModelCheckpoint('model1', verbose=1, save_best_only=True, save_weights_only=True)],\\\n",
    "                    validation_data=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b597a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step - loss: 0.4436 - jaccard_distance: 99.5192\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,jaccard_distance\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,jaccard_distance,lr\n",
      "\n",
      "Epoch 00001: saving model to model1\n"
     ]
    }
   ],
   "source": [
    "results_1 = model1.fit(x_train3[0:10,:,:], y_train3[0:10,:,:], batch_size=10, epochs=1, \\\n",
    "                    callbacks= callbacks + [ModelCheckpoint('model1', verbose=1, save_best_only=False, save_weights_only=True)],\\\n",
    "                    validation_data=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0ac9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3 = np.load('CNN_trainingSet\\X_train.npy')\n",
    "y_train3 = np.load('CNN_trainingSet\\Y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f961d2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 256, 256)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e474d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 256, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d64ec442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 256, 256)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cacef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
